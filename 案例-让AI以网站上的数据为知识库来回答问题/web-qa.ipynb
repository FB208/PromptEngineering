{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://platform.deepseek.com/api-docs/zh-cn/\n",
      "https://platform.deepseek.com/api-docs/zh-cn/api/create-chat-completion\n",
      "https://platform.deepseek.com/api-docs/api/create-chat-completion\n",
      "https://platform.deepseek.com/api-docs/api/deepseek-api\n",
      "https://platform.deepseek.com/downloads/DeepSeek%20Open%20Platform%20Terms%20of%20Service.html\n",
      "https://platform.deepseek.com/api-docs/api/get-user-balance\n",
      "https://platform.deepseek.com/api-docs/pricing\n",
      "https://platform.deepseek.com/api-docs/api/list-models\n",
      "https://platform.deepseek.com/api-docs/faq\n",
      "https://platform.deepseek.com/transactions\n",
      "https://platform.deepseek.com/api-docs/updates\n",
      "https://platform.deepseek.com/api-docs\n",
      "https://platform.deepseek.com/top_up\n",
      "https://platform.deepseek.com/api-docs/zh-cn/api/get-user-balance\n",
      "https://platform.deepseek.com/api_keys\n",
      "https://platform.deepseek.com/api-docs/zh-cn/updates\n",
      "https://platform.deepseek.com/api-docs/zh-cn\n",
      "https://platform.deepseek.com//top_up\n",
      "https://platform.deepseek.com/api-docs/zh-cn/api/list-models\n",
      "https://platform.deepseek.com/api-docs/zh-cn/api/deepseek-api\n",
      "https://platform.deepseek.com/downloads/DeepSeek开放平台用户协议.html\n",
      "'ascii' codec can't encode characters in position 23-30: ordinal not in range(128)\n",
      "https://platform.deepseek.com/downloads/DeepSeek%E5%BC%80%E6%94%BE%E5%B9%B3%E5%8F%B0%E7%94%A8%E6%88%B7%E5%8D%8F%E8%AE%AE.html\n",
      "https://platform.deepseek.com/downloads/DeepSeek开放平台服务协议.html\n",
      "'ascii' codec can't encode characters in position 23-30: ordinal not in range(128)\n",
      "https://platform.deepseek.com/api-docs/zh-cn/pricing\n",
      "https://platform.deepseek.com\n",
      "https://platform.deepseek.com/api-docs/zh-cn/faq\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "# 匹配URL的正则表达式模式\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "# 定义要抓取的网址\n",
    "domain = \"deepseek.com\"\n",
    "full_url = \"https://platform.deepseek.com/api-docs/zh-cn/\"\n",
    "\n",
    "# 创建一个类来解析HTML并获取超链接\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 创建一个数组来存储超链接\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # 重写HTMLParser的handle_starttag方法以获取超链接\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # 如果标签是锚标签并且具有href属性，则将href属性添加到超链接列表中\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n",
    "\n",
    "# 从URL获取超链接的函数\n",
    "def get_hyperlinks(url):\n",
    "    \n",
    "    # 尝试打开URL并阅读超文本标记语言\n",
    "    try:\n",
    "        # 打开URL并阅读内容\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "\n",
    "            # 如果响应不是html，则返回一个空列表\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            \n",
    "            # 解码html\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # 创建html解析器，然后解析html以获取超链接\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks\n",
    "\n",
    "# 从同一域中的URL获取超链接的函数\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # 如果链接是URL，检查它是否在同一域内\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # 解析URL并检查域是否相同\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # 如果链接不是URL，请检查它是否是相对链接\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    # 返回同一域中的超链接列表\n",
    "    return list(set(clean_links))\n",
    "\n",
    "\n",
    "def crawl(url):\n",
    "    # 解析URL并获取域\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # 创建一个队列来存储要抓取的URL\n",
    "    queue = deque([url])\n",
    "\n",
    "    # 创建一个集合来存储已经看到的URL（没有重复）\n",
    "    seen = set([url])\n",
    "\n",
    "    # 创建一个目录来存储文本文件\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # 创建一个目录来存储csv文件\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # 当队列不为空时，继续\n",
    "    while queue:\n",
    "\n",
    "        # 从队列中获取下一个URL\n",
    "        url = queue.pop()\n",
    "        print(url)\n",
    "\n",
    "        # 将文本从url保存到<url>. txt文件\n",
    "        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "            # 使用BeautifulSoup从URL获取文本\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "            # 获取文本但删除标签\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # 如果爬虫到达需要JavaScript的页面，它将停止爬行\n",
    "            if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "            \n",
    "            # 否则，将文本写入文本目录中的文件\n",
    "            f.write(text)\n",
    "\n",
    "        # 从URL获取超链接并将它们添加到队列中\n",
    "        for link in get_domain_hyperlinks(local_domain, url):\n",
    "            if link not in seen:\n",
    "                queue.append(link)\n",
    "                seen.add(link)\n",
    "\n",
    "crawl(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openai.com/\n",
      "HTTP Error 403: Forbidden\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "# 匹配URL的正则表达式模式\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "# 定义要抓取的网址\n",
    "domain = \"openai.com\"\n",
    "full_url = \"https://openai.com/\"\n",
    "\n",
    "# 创建一个类来解析HTML并获取超链接\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 创建一个数组来存储超链接\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # 重写HTMLParser的handle_starttag方法以获取超链接\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # 如果标签是锚标签并且具有href属性，则将href属性添加到超链接列表中\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n",
    "\n",
    "# 从URL获取超链接的函数\n",
    "def get_hyperlinks(url):\n",
    "    \n",
    "    # 尝试打开URL并阅读超文本标记语言\n",
    "    try:\n",
    "        # 打开URL并阅读内容\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "\n",
    "            # 如果响应不是html，则返回一个空列表\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            \n",
    "            # 解码html\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # 创建html解析器，然后解析html以获取超链接\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks\n",
    "\n",
    "# 从同一域中的URL获取超链接的函数\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # 如果链接是URL，检查它是否在同一域内\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # 解析URL并检查域是否相同\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # 如果链接不是URL，请检查它是否是相对链接\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    # 返回同一域中的超链接列表\n",
    "    return list(set(clean_links))\n",
    "\n",
    "\n",
    "def crawl(url):\n",
    "    # 解析URL并获取域\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # 创建一个队列来存储要抓取的URL\n",
    "    queue = deque([url])\n",
    "\n",
    "    # 创建一个集合来存储已经看到的URL（没有重复）\n",
    "    seen = set([url])\n",
    "\n",
    "    # 创建一个目录来存储文本文件\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # 创建一个目录来存储csv文件\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # 当队列不为空时，继续\n",
    "    while queue:\n",
    "\n",
    "        # 从队列中获取下一个URL\n",
    "        url = queue.pop()\n",
    "        print(url)\n",
    "\n",
    "        # 将文本从url保存到<url>. txt文件\n",
    "        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "            # 使用BeautifulSoup从URL获取文本\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "            # 获取文本但删除标签\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # 如果爬虫到达需要JavaScript的页面，它将停止爬行\n",
    "            if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "            \n",
    "            # 否则，将文本写入文本目录中的文件\n",
    "            f.write(text)\n",
    "\n",
    "        # 从URL获取超链接并将它们添加到队列中\n",
    "        for link in get_domain_hyperlinks(local_domain, url):\n",
    "            if link not in seen:\n",
    "                queue.append(link)\n",
    "                seen.add(link)\n",
    "\n",
    "crawl(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openai.com/\n",
      "HTTP Error 403: Forbidden\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "# 匹配URL的正则表达式模式\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "# 定义要抓取的网址\n",
    "domain = \"openai.com\"\n",
    "full_url = \"https://openai.com/\"\n",
    "\n",
    "# 创建一个类来解析HTML并获取超链接\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 创建一个数组来存储超链接\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # 重写HTMLParser的handle_starttag方法以获取超链接\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # 如果标签是锚标签并且具有href属性，则将href属性添加到超链接列表中\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n",
    "\n",
    "# 从URL获取超链接的函数\n",
    "def get_hyperlinks(url):\n",
    "    \n",
    "    # 尝试打开URL并阅读超文本标记语言\n",
    "    try:\n",
    "        # 打开URL并阅读内容\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "\n",
    "            # 如果响应不是html，则返回一个空列表\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            \n",
    "            # 解码html\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # 创建html解析器，然后解析html以获取超链接\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks\n",
    "\n",
    "# 从同一域中的URL获取超链接的函数\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # 如果链接是URL，检查它是否在同一域内\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # 解析URL并检查域是否相同\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # 如果链接不是URL，请检查它是否是相对链接\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    # 返回同一域中的超链接列表\n",
    "    return list(set(clean_links))\n",
    "\n",
    "\n",
    "def crawl(url):\n",
    "    # 解析URL并获取域\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # 创建一个队列来存储要抓取的URL\n",
    "    queue = deque([url])\n",
    "\n",
    "    # 创建一个集合来存储已经看到的URL（没有重复）\n",
    "    seen = set([url])\n",
    "\n",
    "    # 创建一个目录来存储文本文件\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # 创建一个目录来存储csv文件\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # 当队列不为空时，继续\n",
    "    while queue:\n",
    "\n",
    "        # 从队列中获取下一个URL\n",
    "        url = queue.pop()\n",
    "        print(url)\n",
    "\n",
    "        # 将文本从url保存到<url>. txt文件\n",
    "        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "            # 使用BeautifulSoup从URL获取文本\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "            # 获取文本但删除标签\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # 如果爬虫到达需要JavaScript的页面，它将停止爬行\n",
    "            if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "            \n",
    "            # 否则，将文本写入文本目录中的文件\n",
    "            f.write(text)\n",
    "\n",
    "        # 从URL获取超链接并将它们添加到队列中\n",
    "        for link in get_domain_hyperlinks(local_domain, url):\n",
    "            if link not in seen:\n",
    "                queue.append(link)\n",
    "                seen.add(link)\n",
    "\n",
    "crawl(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openai.com/\n",
      "HTTP Error 403: Forbidden\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "# 匹配URL的正则表达式模式\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "# 定义要抓取的网址\n",
    "domain = \"openai.com\"\n",
    "full_url = \"https://openai.com/\"\n",
    "\n",
    "# 创建一个类来解析HTML并获取超链接\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 创建一个数组来存储超链接\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # 重写HTMLParser的handle_starttag方法以获取超链接\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # 如果标签是锚标签并且具有href属性，则将href属性添加到超链接列表中\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n",
    "\n",
    "# 从URL获取超链接的函数\n",
    "def get_hyperlinks(url):\n",
    "    \n",
    "    # 尝试打开URL并阅读超文本标记语言\n",
    "    try:\n",
    "        # 打开URL并阅读内容\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "\n",
    "            # 如果响应不是html，则返回一个空列表\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            \n",
    "            # 解码html\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # 创建html解析器，然后解析html以获取超链接\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks\n",
    "\n",
    "# 从同一域中的URL获取超链接的函数\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # 如果链接是URL，检查它是否在同一域内\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # 解析URL并检查域是否相同\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # 如果链接不是URL，请检查它是否是相对链接\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    # 返回同一域中的超链接列表\n",
    "    return list(set(clean_links))\n",
    "\n",
    "\n",
    "def crawl(url):\n",
    "    # 解析URL并获取域\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # 创建一个队列来存储要抓取的URL\n",
    "    queue = deque([url])\n",
    "\n",
    "    # 创建一个集合来存储已经看到的URL（没有重复）\n",
    "    seen = set([url])\n",
    "\n",
    "    # 创建一个目录来存储文本文件\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # 创建一个目录来存储csv文件\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # 当队列不为空时，继续\n",
    "    while queue:\n",
    "\n",
    "        # 从队列中获取下一个URL\n",
    "        url = queue.pop()\n",
    "        print(url)\n",
    "\n",
    "        # 将文本从url保存到<url>. txt文件\n",
    "        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "            # 使用BeautifulSoup从URL获取文本\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "            # 获取文本但删除标签\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # 如果爬虫到达需要JavaScript的页面，它将停止爬行\n",
    "            if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "            \n",
    "            # 否则，将文本写入文本目录中的文件\n",
    "            f.write(text)\n",
    "\n",
    "        # 从URL获取超链接并将它们添加到队列中\n",
    "        for link in get_domain_hyperlinks(local_domain, url):\n",
    "            if link not in seen:\n",
    "                queue.append(link)\n",
    "                seen.add(link)\n",
    "\n",
    "crawl(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openai.com/\n",
      "HTTP Error 403: Forbidden\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "# 匹配URL的正则表达式模式\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "# 定义要抓取的网址\n",
    "domain = \"openai.com\"\n",
    "full_url = \"https://openai.com/\"\n",
    "\n",
    "# 创建一个类来解析HTML并获取超链接\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 创建一个数组来存储超链接\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # 重写HTMLParser的handle_starttag方法以获取超链接\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # 如果标签是锚标签并且具有href属性，则将href属性添加到超链接列表中\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n",
    "\n",
    "# 从URL获取超链接的函数\n",
    "def get_hyperlinks(url):\n",
    "    \n",
    "    # 尝试打开URL并阅读超文本标记语言\n",
    "    try:\n",
    "        # 打开URL并阅读内容\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "\n",
    "            # 如果响应不是html，则返回一个空列表\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            \n",
    "            # 解码html\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # 创建html解析器，然后解析html以获取超链接\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks\n",
    "\n",
    "# 从同一域中的URL获取超链接的函数\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # 如果链接是URL，检查它是否在同一域内\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # 解析URL并检查域是否相同\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # 如果链接不是URL，请检查它是否是相对链接\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    # 返回同一域中的超链接列表\n",
    "    return list(set(clean_links))\n",
    "\n",
    "\n",
    "def crawl(url):\n",
    "    # 解析URL并获取域\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # 创建一个队列来存储要抓取的URL\n",
    "    queue = deque([url])\n",
    "\n",
    "    # 创建一个集合来存储已经看到的URL（没有重复）\n",
    "    seen = set([url])\n",
    "\n",
    "    # 创建一个目录来存储文本文件\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # 创建一个目录来存储csv文件\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # 当队列不为空时，继续\n",
    "    while queue:\n",
    "\n",
    "        # 从队列中获取下一个URL\n",
    "        url = queue.pop()\n",
    "        print(url)\n",
    "\n",
    "        # 将文本从url保存到<url>. txt文件\n",
    "        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "            # 使用BeautifulSoup从URL获取文本\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "            # 获取文本但删除标签\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # 如果爬虫到达需要JavaScript的页面，它将停止爬行\n",
    "            if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "            \n",
    "            # 否则，将文本写入文本目录中的文件\n",
    "            f.write(text)\n",
    "\n",
    "        # 从URL获取超链接并将它们添加到队列中\n",
    "        for link in get_domain_hyperlinks(local_domain, url):\n",
    "            if link not in seen:\n",
    "                queue.append(link)\n",
    "                seen.add(link)\n",
    "\n",
    "crawl(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openai.com/\n",
      "HTTP Error 403: Forbidden\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "# 匹配URL的正则表达式模式\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "# 定义要抓取的网址\n",
    "domain = \"openai.com\"\n",
    "full_url = \"https://openai.com/\"\n",
    "\n",
    "# 创建一个类来解析HTML并获取超链接\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 创建一个数组来存储超链接\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # 重写HTMLParser的handle_starttag方法以获取超链接\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # 如果标签是锚标签并且具有href属性，则将href属性添加到超链接列表中\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n",
    "\n",
    "# 从URL获取超链接的函数\n",
    "def get_hyperlinks(url):\n",
    "    \n",
    "    # 尝试打开URL并阅读超文本标记语言\n",
    "    try:\n",
    "        # 打开URL并阅读内容\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "\n",
    "            # 如果响应不是html，则返回一个空列表\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            \n",
    "            # 解码html\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # 创建html解析器，然后解析html以获取超链接\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks\n",
    "\n",
    "# 从同一域中的URL获取超链接的函数\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # 如果链接是URL，检查它是否在同一域内\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # 解析URL并检查域是否相同\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # 如果链接不是URL，请检查它是否是相对链接\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    # 返回同一域中的超链接列表\n",
    "    return list(set(clean_links))\n",
    "\n",
    "\n",
    "def crawl(url):\n",
    "    # 解析URL并获取域\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # 创建一个队列来存储要抓取的URL\n",
    "    queue = deque([url])\n",
    "\n",
    "    # 创建一个集合来存储已经看到的URL（没有重复）\n",
    "    seen = set([url])\n",
    "\n",
    "    # 创建一个目录来存储文本文件\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # 创建一个目录来存储csv文件\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # 当队列不为空时，继续\n",
    "    while queue:\n",
    "\n",
    "        # 从队列中获取下一个URL\n",
    "        url = queue.pop()\n",
    "        print(url)\n",
    "\n",
    "        # 将文本从url保存到<url>. txt文件\n",
    "        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "            # 使用BeautifulSoup从URL获取文本\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "            # 获取文本但删除标签\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # 如果爬虫到达需要JavaScript的页面，它将停止爬行\n",
    "            if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "            \n",
    "            # 否则，将文本写入文本目录中的文件\n",
    "            f.write(text)\n",
    "\n",
    "        # 从URL获取超链接并将它们添加到队列中\n",
    "        for link in get_domain_hyperlinks(local_domain, url):\n",
    "            if link not in seen:\n",
    "                queue.append(link)\n",
    "                seen.add(link)\n",
    "\n",
    "crawl(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openai.com/\n",
      "HTTP Error 403: Forbidden\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "# 匹配URL的正则表达式模式\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "# 定义要抓取的网址\n",
    "domain = \"openai.com\"\n",
    "full_url = \"https://openai.com/\"\n",
    "\n",
    "# 创建一个类来解析HTML并获取超链接\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 创建一个数组来存储超链接\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # 重写HTMLParser的handle_starttag方法以获取超链接\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # 如果标签是锚标签并且具有href属性，则将href属性添加到超链接列表中\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n",
    "\n",
    "# 从URL获取超链接的函数\n",
    "def get_hyperlinks(url):\n",
    "    \n",
    "    # 尝试打开URL并阅读超文本标记语言\n",
    "    try:\n",
    "        # 打开URL并阅读内容\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "\n",
    "            # 如果响应不是html，则返回一个空列表\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            \n",
    "            # 解码html\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # 创建html解析器，然后解析html以获取超链接\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks\n",
    "\n",
    "# 从同一域中的URL获取超链接的函数\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # 如果链接是URL，检查它是否在同一域内\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # 解析URL并检查域是否相同\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # 如果链接不是URL，请检查它是否是相对链接\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    # 返回同一域中的超链接列表\n",
    "    return list(set(clean_links))\n",
    "\n",
    "\n",
    "def crawl(url):\n",
    "    # 解析URL并获取域\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # 创建一个队列来存储要抓取的URL\n",
    "    queue = deque([url])\n",
    "\n",
    "    # 创建一个集合来存储已经看到的URL（没有重复）\n",
    "    seen = set([url])\n",
    "\n",
    "    # 创建一个目录来存储文本文件\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # 创建一个目录来存储csv文件\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # 当队列不为空时，继续\n",
    "    while queue:\n",
    "\n",
    "        # 从队列中获取下一个URL\n",
    "        url = queue.pop()\n",
    "        print(url)\n",
    "\n",
    "        # 将文本从url保存到<url>. txt文件\n",
    "        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "            # 使用BeautifulSoup从URL获取文本\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "            # 获取文本但删除标签\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # 如果爬虫到达需要JavaScript的页面，它将停止爬行\n",
    "            if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "            \n",
    "            # 否则，将文本写入文本目录中的文件\n",
    "            f.write(text)\n",
    "\n",
    "        # 从URL获取超链接并将它们添加到队列中\n",
    "        for link in get_domain_hyperlinks(local_domain, url):\n",
    "            if link not in seen:\n",
    "                queue.append(link)\n",
    "                seen.add(link)\n",
    "\n",
    "crawl(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openai.com/\n",
      "HTTP Error 403: Forbidden\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "# 匹配URL的正则表达式模式\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "# 定义要抓取的网址\n",
    "domain = \"openai.com\"\n",
    "full_url = \"https://openai.com/\"\n",
    "\n",
    "# 创建一个类来解析HTML并获取超链接\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 创建一个数组来存储超链接\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # 重写HTMLParser的handle_starttag方法以获取超链接\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # 如果标签是锚标签并且具有href属性，则将href属性添加到超链接列表中\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n",
    "\n",
    "# 从URL获取超链接的函数\n",
    "def get_hyperlinks(url):\n",
    "    \n",
    "    # 尝试打开URL并阅读超文本标记语言\n",
    "    try:\n",
    "        # 打开URL并阅读内容\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "\n",
    "            # 如果响应不是html，则返回一个空列表\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            \n",
    "            # 解码html\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # 创建html解析器，然后解析html以获取超链接\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks\n",
    "\n",
    "# 从同一域中的URL获取超链接的函数\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # 如果链接是URL，检查它是否在同一域内\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # 解析URL并检查域是否相同\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # 如果链接不是URL，请检查它是否是相对链接\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    # 返回同一域中的超链接列表\n",
    "    return list(set(clean_links))\n",
    "\n",
    "\n",
    "def crawl(url):\n",
    "    # 解析URL并获取域\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # 创建一个队列来存储要抓取的URL\n",
    "    queue = deque([url])\n",
    "\n",
    "    # 创建一个集合来存储已经看到的URL（没有重复）\n",
    "    seen = set([url])\n",
    "\n",
    "    # 创建一个目录来存储文本文件\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # 创建一个目录来存储csv文件\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # 当队列不为空时，继续\n",
    "    while queue:\n",
    "\n",
    "        # 从队列中获取下一个URL\n",
    "        url = queue.pop()\n",
    "        print(url)\n",
    "\n",
    "        # 将文本从url保存到<url>. txt文件\n",
    "        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "            # 使用BeautifulSoup从URL获取文本\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "            # 获取文本但删除标签\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # 如果爬虫到达需要JavaScript的页面，它将停止爬行\n",
    "            if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "            \n",
    "            # 否则，将文本写入文本目录中的文件\n",
    "            f.write(text)\n",
    "\n",
    "        # 从URL获取超链接并将它们添加到队列中\n",
    "        for link in get_domain_hyperlinks(local_domain, url):\n",
    "            if link not in seen:\n",
    "                queue.append(link)\n",
    "                seen.add(link)\n",
    "\n",
    "crawl(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openai.com/\n",
      "HTTP Error 403: Forbidden\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "# 匹配URL的正则表达式模式\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "# 定义要抓取的网址\n",
    "domain = \"openai.com\"\n",
    "full_url = \"https://openai.com/\"\n",
    "\n",
    "# 创建一个类来解析HTML并获取超链接\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 创建一个数组来存储超链接\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # 重写HTMLParser的handle_starttag方法以获取超链接\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # 如果标签是锚标签并且具有href属性，则将href属性添加到超链接列表中\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n",
    "\n",
    "# 从URL获取超链接的函数\n",
    "def get_hyperlinks(url):\n",
    "    \n",
    "    # 尝试打开URL并阅读超文本标记语言\n",
    "    try:\n",
    "        # 打开URL并阅读内容\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "\n",
    "            # 如果响应不是html，则返回一个空列表\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            \n",
    "            # 解码html\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # 创建html解析器，然后解析html以获取超链接\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks\n",
    "\n",
    "# 从同一域中的URL获取超链接的函数\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # 如果链接是URL，检查它是否在同一域内\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # 解析URL并检查域是否相同\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # 如果链接不是URL，请检查它是否是相对链接\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    # 返回同一域中的超链接列表\n",
    "    return list(set(clean_links))\n",
    "\n",
    "\n",
    "def crawl(url):\n",
    "    # 解析URL并获取域\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # 创建一个队列来存储要抓取的URL\n",
    "    queue = deque([url])\n",
    "\n",
    "    # 创建一个集合来存储已经看到的URL（没有重复）\n",
    "    seen = set([url])\n",
    "\n",
    "    # 创建一个目录来存储文本文件\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # 创建一个目录来存储csv文件\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # 当队列不为空时，继续\n",
    "    while queue:\n",
    "\n",
    "        # 从队列中获取下一个URL\n",
    "        url = queue.pop()\n",
    "        print(url)\n",
    "\n",
    "        # 将文本从url保存到<url>. txt文件\n",
    "        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "            # 使用BeautifulSoup从URL获取文本\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "            # 获取文本但删除标签\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # 如果爬虫到达需要JavaScript的页面，它将停止爬行\n",
    "            if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "            \n",
    "            # 否则，将文本写入文本目录中的文件\n",
    "            f.write(text)\n",
    "\n",
    "        # 从URL获取超链接并将它们添加到队列中\n",
    "        for link in get_domain_hyperlinks(local_domain, url):\n",
    "            if link not in seen:\n",
    "                queue.append(link)\n",
    "                seen.add(link)\n",
    "\n",
    "crawl(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openai.com/\n",
      "HTTP Error 403: Forbidden\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "# 匹配URL的正则表达式模式\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "# 定义要抓取的网址\n",
    "domain = \"openai.com\"\n",
    "full_url = \"https://openai.com/\"\n",
    "\n",
    "# 创建一个类来解析HTML并获取超链接\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 创建一个数组来存储超链接\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # 重写HTMLParser的handle_starttag方法以获取超链接\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # 如果标签是锚标签并且具有href属性，则将href属性添加到超链接列表中\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n",
    "\n",
    "# 从URL获取超链接的函数\n",
    "def get_hyperlinks(url):\n",
    "    \n",
    "    # 尝试打开URL并阅读超文本标记语言\n",
    "    try:\n",
    "        # 打开URL并阅读内容\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "\n",
    "            # 如果响应不是html，则返回一个空列表\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            \n",
    "            # 解码html\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # 创建html解析器，然后解析html以获取超链接\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks\n",
    "\n",
    "# 从同一域中的URL获取超链接的函数\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # 如果链接是URL，检查它是否在同一域内\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # 解析URL并检查域是否相同\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # 如果链接不是URL，请检查它是否是相对链接\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    # 返回同一域中的超链接列表\n",
    "    return list(set(clean_links))\n",
    "\n",
    "\n",
    "def crawl(url):\n",
    "    # 解析URL并获取域\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # 创建一个队列来存储要抓取的URL\n",
    "    queue = deque([url])\n",
    "\n",
    "    # 创建一个集合来存储已经看到的URL（没有重复）\n",
    "    seen = set([url])\n",
    "\n",
    "    # 创建一个目录来存储文本文件\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # 创建一个目录来存储csv文件\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # 当队列不为空时，继续\n",
    "    while queue:\n",
    "\n",
    "        # 从队列中获取下一个URL\n",
    "        url = queue.pop()\n",
    "        print(url)\n",
    "\n",
    "        # 将文本从url保存到<url>. txt文件\n",
    "        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "            # 使用BeautifulSoup从URL获取文本\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "            # 获取文本但删除标签\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # 如果爬虫到达需要JavaScript的页面，它将停止爬行\n",
    "            if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "            \n",
    "            # 否则，将文本写入文本目录中的文件\n",
    "            f.write(text)\n",
    "\n",
    "        # 从URL获取超链接并将它们添加到队列中\n",
    "        for link in get_domain_hyperlinks(local_domain, url):\n",
    "            if link not in seen:\n",
    "                queue.append(link)\n",
    "                seen.add(link)\n",
    "\n",
    "crawl(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openai.com/\n",
      "HTTP Error 403: Forbidden\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "# 匹配URL的正则表达式模式\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "# 定义要抓取的网址\n",
    "domain = \"openai.com\"\n",
    "full_url = \"https://openai.com/\"\n",
    "\n",
    "# 创建一个类来解析HTML并获取超链接\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 创建一个数组来存储超链接\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # 重写HTMLParser的handle_starttag方法以获取超链接\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # 如果标签是锚标签并且具有href属性，则将href属性添加到超链接列表中\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n",
    "\n",
    "# 从URL获取超链接的函数\n",
    "def get_hyperlinks(url):\n",
    "    \n",
    "    # 尝试打开URL并阅读超文本标记语言\n",
    "    try:\n",
    "        # 打开URL并阅读内容\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "\n",
    "            # 如果响应不是html，则返回一个空列表\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            \n",
    "            # 解码html\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # 创建html解析器，然后解析html以获取超链接\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks\n",
    "\n",
    "# 从同一域中的URL获取超链接的函数\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # 如果链接是URL，检查它是否在同一域内\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # 解析URL并检查域是否相同\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # 如果链接不是URL，请检查它是否是相对链接\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    # 返回同一域中的超链接列表\n",
    "    return list(set(clean_links))\n",
    "\n",
    "\n",
    "def crawl(url):\n",
    "    # 解析URL并获取域\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # 创建一个队列来存储要抓取的URL\n",
    "    queue = deque([url])\n",
    "\n",
    "    # 创建一个集合来存储已经看到的URL（没有重复）\n",
    "    seen = set([url])\n",
    "\n",
    "    # 创建一个目录来存储文本文件\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # 创建一个目录来存储csv文件\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # 当队列不为空时，继续\n",
    "    while queue:\n",
    "\n",
    "        # 从队列中获取下一个URL\n",
    "        url = queue.pop()\n",
    "        print(url)\n",
    "\n",
    "        # 将文本从url保存到<url>. txt文件\n",
    "        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "            # 使用BeautifulSoup从URL获取文本\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "            # 获取文本但删除标签\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # 如果爬虫到达需要JavaScript的页面，它将停止爬行\n",
    "            if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "            \n",
    "            # 否则，将文本写入文本目录中的文件\n",
    "            f.write(text)\n",
    "\n",
    "        # 从URL获取超链接并将它们添加到队列中\n",
    "        for link in get_domain_hyperlinks(local_domain, url):\n",
    "            if link not in seen:\n",
    "                queue.append(link)\n",
    "                seen.add(link)\n",
    "\n",
    "crawl(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openai.com/\n",
      "HTTP Error 403: Forbidden\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "# 匹配URL的正则表达式模式\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "# 定义要抓取的网址\n",
    "domain = \"openai.com\"\n",
    "full_url = \"https://openai.com/\"\n",
    "\n",
    "# 创建一个类来解析HTML并获取超链接\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 创建一个数组来存储超链接\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # 重写HTMLParser的handle_starttag方法以获取超链接\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # 如果标签是锚标签并且具有href属性，则将href属性添加到超链接列表中\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n",
    "\n",
    "# 从URL获取超链接的函数\n",
    "def get_hyperlinks(url):\n",
    "    \n",
    "    # 尝试打开URL并阅读超文本标记语言\n",
    "    try:\n",
    "        # 打开URL并阅读内容\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "\n",
    "            # 如果响应不是html，则返回一个空列表\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            \n",
    "            # 解码html\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # 创建html解析器，然后解析html以获取超链接\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks\n",
    "\n",
    "# 从同一域中的URL获取超链接的函数\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # 如果链接是URL，检查它是否在同一域内\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # 解析URL并检查域是否相同\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # 如果链接不是URL，请检查它是否是相对链接\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    # 返回同一域中的超链接列表\n",
    "    return list(set(clean_links))\n",
    "\n",
    "\n",
    "def crawl(url):\n",
    "    # 解析URL并获取域\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # 创建一个队列来存储要抓取的URL\n",
    "    queue = deque([url])\n",
    "\n",
    "    # 创建一个集合来存储已经看到的URL（没有重复）\n",
    "    seen = set([url])\n",
    "\n",
    "    # 创建一个目录来存储文本文件\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # 创建一个目录来存储csv文件\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # 当队列不为空时，继续\n",
    "    while queue:\n",
    "\n",
    "        # 从队列中获取下一个URL\n",
    "        url = queue.pop()\n",
    "        print(url)\n",
    "\n",
    "        # 将文本从url保存到<url>. txt文件\n",
    "        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "            # 使用BeautifulSoup从URL获取文本\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "            # 获取文本但删除标签\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # 如果爬虫到达需要JavaScript的页面，它将停止爬行\n",
    "            if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "            \n",
    "            # 否则，将文本写入文本目录中的文件\n",
    "            f.write(text)\n",
    "\n",
    "        # 从URL获取超链接并将它们添加到队列中\n",
    "        for link in get_domain_hyperlinks(local_domain, url):\n",
    "            if link not in seen:\n",
    "                queue.append(link)\n",
    "                seen.add(link)\n",
    "\n",
    "crawl(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newlines(serie):\n",
    "    \"\"\"\n",
    "    移除字符串中的换行符。\n",
    "    \n",
    "    参数:\n",
    "    serie: pandas序列，包含需要处理的字符串。\n",
    "    \n",
    "    返回:\n",
    "    处理后的pandas序列，其中的换行符已被替换为单个空格。\n",
    "    \"\"\"\n",
    "    # 替换UNIX风格的换行符\n",
    "    serie = serie.str.replace('\\n', ' ')\n",
    "    # 替换Windows风格的换行符\n",
    "    serie = serie.str.replace('\\\\n', ' ')\n",
    "    # 压缩连续的两个空格为一个，以避免不必要的空格\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    # 再次压缩，确保所有的连续空格都被处理，这是对前一步的保险措施\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    return serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 创建一个列表来存储文本文件\n",
    "texts=[]\n",
    "\n",
    "# 遍历指定域名下的所有文本文件\n",
    "for file in os.listdir(\"text/\" + domain + \"/\"):\n",
    "    # 以只读模式打开文件\n",
    "    with open(\"text/\" + domain + \"/\" + file, \"r\") as f:\n",
    "        # 读取文件内容\n",
    "        text = f.read()\n",
    "        # 处理文件名，去除前导字符和尾部字符，并替换特定字符为空格\n",
    "        # 这是为了规范化文件名，使其更适合后续处理\n",
    "        processed_file_name = file[11:-4].replace('-',' ').replace('_', ' ').replace('#update','')\n",
    "        # 将文件名和内容作为一个元组添加到列表中\n",
    "        texts.append((processed_file_name, text))\n",
    "\n",
    "# 使用列表创建一个DataFrame，指定列名为'fname'和'text'\n",
    "df = pd.DataFrame(texts, columns = ['fname', 'text'])\n",
    "\n",
    "# 合并文件名和内容，去除换行符，并更新文本列\n",
    "df['text'] = df.fname + \". \" + remove_newlines(df.text)\n",
    "# 将处理后的数据保存到CSV文件中\n",
    "df.to_csv('processed/scraped.csv')\n",
    "# 显示DataFrame的前5行，用于检查数据是否正确处理\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# 加载设计用于ada-002模型的cl100k_base标记器\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "df = pd.read_csv('processed/scraped.csv', index_col=0)\n",
    "df.columns = ['title', 'text']\n",
    "\n",
    "# 标记文本并将标记数保存到新列\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
    "\n",
    "# 使用直方图可视化每行标记数量的分布\n",
    "df.n_tokens.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 500\n",
    "\n",
    "# 将文本拆分为最大标记数的块的函数\n",
    "def split_into_many(text, max_tokens = max_tokens):\n",
    "\n",
    "    sentences = text.split('. ')\n",
    "\n",
    "    # 获取每个句子的标记数\n",
    "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "    \n",
    "    chunks = []\n",
    "    tokens_so_far = 0\n",
    "    chunk = []\n",
    "\n",
    "    # 循环遍历连接在元组中的句子和标记\n",
    "    for sentence, token in zip(sentences, n_tokens):\n",
    "\n",
    "        # 如果到目前为止的标记数加上当前句子中的标记数更大\n",
    "        # 超过令牌的最大数量，然后将块添加到块列表并重置到目前为止的块和令牌\n",
    "        if tokens_so_far + token > max_tokens:\n",
    "            chunks.append(\". \".join(chunk) + \".\")\n",
    "            chunk = []\n",
    "            tokens_so_far = 0\n",
    "\n",
    "        #如果当前句子中的标记数大于最大标记数，则转到下一句\n",
    "        if token > max_tokens:\n",
    "            continue\n",
    "\n",
    "        #否则，将句子添加到块中，并将标记数添加到总数中\n",
    "        chunk.append(sentence)\n",
    "        tokens_so_far += token + 1\n",
    "\n",
    "    #将最后一个块添加到块列表中\n",
    "    if chunk:\n",
    "        chunks.append(\". \".join(chunk) + \".\")\n",
    "\n",
    "    return chunks\n",
    "    \n",
    "\n",
    "shortened = []\n",
    "\n",
    "#循环通过数据帧\n",
    "for row in df.iterrows():\n",
    "\n",
    "    if row[1]['text'] is None:\n",
    "        continue\n",
    "\n",
    "    #如果标记数大于最大标记数，则将文本拆分为块\n",
    "    if row[1]['n_tokens'] > max_tokens:\n",
    "        shortened += split_into_many(row[1]['text'])\n",
    "    \n",
    "    #否则，将文本添加到缩短文本列表中\n",
    "    else:\n",
    "        shortened.append( row[1]['text'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(shortened, columns = ['text'])\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
    "df.n_tokens.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "df['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'])\n",
    "df.to_csv('processed/embeddings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from openai.embeddings_utils import distances_from_embeddings, cosine_similarity\n",
    "\n",
    "df=pd.read_csv('processed/embeddings.csv', index_col=0)\n",
    "df['embeddings'] = df['embeddings'].apply(literal_eval).apply(np.array)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context(\n",
    "    question, df, max_len=1800, size=\"ada\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a context for a question by finding the most similar context from the dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    #获取问题的嵌入\n",
    "    q_embeddings = openai.Embedding.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']\n",
    "\n",
    "    #获取与嵌入的距离\n",
    "    df['distances'] = distances_from_embeddings(q_embeddings, df['embeddings'].values, distance_metric='cosine')\n",
    "\n",
    "\n",
    "    returns = []\n",
    "    cur_len = 0\n",
    "\n",
    "    #按距离排序并将文本添加到上下文中，直到上下文太长\n",
    "    for i, row in df.sort_values('distances', ascending=True).iterrows():\n",
    "        \n",
    "        #将文本的长度添加到当前长度\n",
    "        cur_len += row['n_tokens'] + 4\n",
    "        \n",
    "        #如果上下文太长，中断\n",
    "        if cur_len > max_len:\n",
    "            break\n",
    "        \n",
    "        #否则将其添加到正在返回的文本中\n",
    "        returns.append(row[\"text\"])\n",
    "\n",
    "    #返回上下文\n",
    "    return \"\\n\\n###\\n\\n\".join(returns)\n",
    "\n",
    "def answer_question(\n",
    "    df,\n",
    "    model=\"text-davinci-003\",\n",
    "    question=\"Am I allowed to publish model outputs to Twitter, without a human review?\",\n",
    "    max_len=1800,\n",
    "    size=\"ada\",\n",
    "    debug=False,\n",
    "    max_tokens=150,\n",
    "    stop_sequence=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Answer a question based on the most similar context from the dataframe texts\n",
    "    \"\"\"\n",
    "    context = create_context(\n",
    "        question,\n",
    "        df,\n",
    "        max_len=max_len,\n",
    "        size=size,\n",
    "    )\n",
    "    #如果调试，打印原始模型响应\n",
    "    if debug:\n",
    "        print(\"Context:\\n\" + context)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        #使用问题和上下文创建完成\n",
    "        response = openai.Completion.create(\n",
    "            prompt=f\"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\nContext: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:\",\n",
    "            temperature=0,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=stop_sequence,\n",
    "            model=model,\n",
    "        )\n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question(df, question=\"What day is it?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question(df, question=\"What is our newest embeddings model?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05f34a34d73b71652304030c1097be3a5720ea2447153dd6542d145a26b73181"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
